%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter4.tex
%% NOVA thesis document file
%%
%% Chapter with lots of dummy text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter4.tex}%

\chapter{Related Work}
\label{cha:related_work}

As mentioned in Section 1, this work will encompass the development of
mechanisms for evaluating and managing the soundness of updates in RESTfull service contracts.
We now summarize the most significant pieces of work from the literature
that relate to the evolution of microservices and their APIs.

\section{Web API Description Languages} % (fold)
\label{sec:web_api_description_languages}

Web API development has risen dramatically in recent years;  unlike statically linked library APIs,
where developers may choose to stick with an earlier version that met their needs, with web APIs,
the provider can discontinue a specific version and capability at any moment.
This represents a heavy burden for client developers as it causes an endless struggle to keep up
with changes pushed by the web API providers; this load is exacerbated if the web API is inadequately documented.

Web API Description Languages (WADL) are domain-specific languages used to describe web service contracts in a standardized structure;
also sometimes referred to as interface description languages (IDLs).

These structured descriptions might be used to produce documentation for human programmers, that is easier to read than free-form documentation,
since all documentation generated by the same tool adheres to the same formatting norms.
Furthermore, description languages are often accurate enough to allow for the automatic generation of various software artifacts such as mock servers,
stub code, load test scripts, and so on.

The OpenAPI Specification (OAS) is the most widely adopted Restful API Description Language;
it defines a standard, language-agnostic interface that allows both humans and computers to discover and
understand the capabilities of a service without access to source code, documentation, or network traffic inspection.
When properly defined, a consumer can understand and interact with the remote service with a minimal amount of implementation logic.


\section{Serialization Frameworks} % (fold)
\label{sec:serialization_frameworks}

Serialization is the process of encoding system state into a standardized format so that it can be delivered across a network or stored with durability.
For serialization, each language usually provides a corresponding library, such as Java serialization.
In the setting of service oriented computing, serialization libraries supplied by programming languages are typically not used to encode messages between services,
because each service may be written in a different language. As a result, data consumers will be unable to comprehend data producers.
Cross-language serialization libraries, such as JSON, can solve this problem.
However, formats such as JSON lack a strictly defined structure,
making data consumption more difficult due to poor type-safety guarantees, and the ability for fields to be unilaterally added or withdrawn at any moment without the consumers' knowledge.
What's missing is a "schema" for data exchange between producers and consumers, akin to an API contract.

The benefit of having a schema is that it explicitly defines the data's structure, type, and meaning.
There have been a few cross-language serialization frameworks that require the data structure to be properly described via schemas.
Avro, Thrift, and Protocol Buffers \cite{8,9,10} are among them.

These frameworks support schema evolution, but only to a limited extent \cite{11}:
\begin{itemize}
    \item Only the addition, removal and renaming of fields is allowed.
    \item To ensure backwards compatibility, added fields must be optional.
    \item The only fields that can be removed to maintain forward compatibility are optional fields.
    \item To provide both forward and backward compatibility, removed and added fields must be optional.
\end{itemize}
Backward compatibility refers to a consumer using schema X to process data produced by schema X-1,
whereas forward compatibility refers to data produced by schema X being read by consumers using schema X-1.

The need for optional fields in the above cases is due to a lack of information on the consumer, typically this information is supplied with the use of default values.
In order to support required fields with backwards-forwards compatibility, the system should enforce required fields when the consumer and producer agree on the schema,
and only use default values when the consumer and producer disagree on the schema.
These limitations are problematic because, in order to enforce required fields in former case,
validation logic would need to be written repeatedly by programmers (in the same layer as the business logic).
This validation should be in a layer above because, the scale of the validation logic is proportional to the complexity of messages being validated;
for messages with more properties, particularly those with nested objects, the validation footprint can rise dramatically in terms of both line-count and logical complexity.

The aforementioned frameworks include built-in support for event-driven and RPC communication methods,
however, if a RESTful communication approach is adopted, these frameworks will be unable to manage changes in the signature of endpoint
(eg. changing the method of REST endpoint's from GET to POST); only changes in record schemas will be managed in this case.
To decrease the complexity of the evolution adaptation process, we believe it is necessary to handle both the evolution of records schemas, and the evolution of API signatures in a single integrated approach.

\section{Schema Registry} % (fold)
\label{sec:schema_registry}

A schema registry, as the name implies, is a repository for schemas.
It stores a versioned history of schemas and provides an interface for retrieving, registering, as well as checking the compliance of schemas.
It is essentially a CRUD (Create, Read, Update, Delete) application with a RESTful API and persistent storage for schema definitions, where each schema is given a unique ID.

Schema registries are commonly used in situations where data consumers need to know the structure of the data written by producers at runtime.
A producer could send its schema to consumers along with the response to a request;
however, this is usually a bad idea because it would result in duplicating functionality across all services, making the system more difficult to maintain.

One framework that makes use of this system is Avro \cite{8}, a data serialization framework developed within Apache's Hadoop project.
The serialized byte sequence of each record in Avro does not include field metadata such as the field name or a tag, but merely the field value, allowing for a more compact serialization method.
All field values are appended back to back, in the same order as they appear in the schema \ref{fig:avro}.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=1.5in]{avro}
    \caption{A Avro schema and its associated serialized record }
    \label{fig:avro}
\end{figure}

The deserializer knows which bytes belong to which field by comparing both the reader and writer schemas and by matching fields by their names.

There are other systems that make use of registry's such a Java RMI \cite{12} that uses schema registry's
to perform remote method invocation, the object-oriented equivalent of remote procedure calls, with support for direct
transfer of serialized Java classes and distributed garbage-collection.
Our approach will need the use of a service registry, which will enhance the capabilities of a schema registry by storing not just schemas but also service-to-service dependencies.

\section{Service Integration Adapters} % (fold)
\label{sec:service_integration_adapters}

There has been substantial investigation on the adaptability of services in the context of service-oriented architectures.
Although SOA \cite{7} and Microservices \cite{14} are similar in that they both use a separation approach based on services,
these two designs differ in several important ways, most notably in that microservices are loosely coupled whereas SOA services are tightly tied due to common data storage between services.
As a result, adaptability in SOA is focused on service integration and replaceability rather than evolution.

In the setting of SOA, adapters are used to wrap the various services which are in general heterogeneous,
(communicate with different protocols and support different data formats) so that they can appear as homogeneous and therefore easier to be integrated.

These adapters presume that data adaptation has already been established by developers at an upper layer,
their primary concern is merging services with different communication protocols and mismatches between operations that have the
same functionality but differ in operation name, number, order or type of operation input/output parameters.

The paper \cite{15} presents a taxonomy of all conceivable mismatches as well as remedies to each type in the context of SOA.
Some of the mismatches defined remain important in the context of microservice evolution:

\paragraph{Message Split Mismatch}
This type of differences occurs when the protocol P requires a single message to achieve certain functionality,
while in protocol PR the same behavior is achieved by receiving several messages.

\paragraph{Message Merge Mismatch}
This type of differences occurs when protocol P needs to receive several messages for achieving certain
functionality while protocol PR requires one message to achieve the same functionality.

\paragraph{Differences at the Operation Level}
This type of mismatch occurs when the operation O of S imposes constraints on input parameters,
which are less restrictive than those of OR input parameters in SR (e.g., differences in value ranges).

\paragraph{}

The two first types of mismatches can be handled in the context of microservice evolution without the use of adaptors.
Essentially, the old endpoints are kept operational but marked as deprecated until there are no more consumers using them, and the new endpoints are provided in parallel.

The latter form of mismatch has an impact on microservice evolution only if service contracts allow refined types.
In other words, when constraints in input parameter are validated in a layer above the application layer.
If validation is conducted in a lower layer, the mismatch can be easily resolved by modifying the application's implementation while keeping the service contract untouched.

\section{Chain of adapters} % (fold)
\label{sec:chain_of_adapters}

The chain of adapters is a design strategy for providing a web service in the face of independently developed unsupervised clients while preserving strict backwards compatibility.
This approach decomposes long update/rollback transactions into smaller, independent transactions.
This technique is discussed in depth in the work \cite{13}. The key concepts of the approach are presented below.

When a new service API is published, the old API is not decommissioned;
instead, it is made available in a different namespace;
in a RESTful system, this often entails supporting operations in different versions.
Endpoints belonging to the API in v1.0 can be made accessible via the path http://example.system/v1,
whereas endpoints belonging to the API in v2.0 are accessible via the path http://example.system/v2.

The previous API implementations are replaced by adapters that redirect all calls to the next API version implementation and also translate data structures as necessary.

Updating a service's API in this manner requires the programmer to do two additional steps:
\begin{itemize}
    \item Duplicate all modified endpoints in service contract into a different namespace.
    \item Implement an adapter that translates the endpoints in version vi->vi+1.
\end{itemize}

The same web service that supports the revised service endpoints also supports the older endpoints that require adapters.
It is not necessary to deploy the web service in several versions.

When a consumer is using version v1, and the producer is using version v4, the endpoints that provide the service in version v1 use the adapters v1->v2, v2->v3, and v3->v4 to translate the operation before forwarding it to the endpoint in the current version.

The service's backwards compatibility is ensured by the composition of adapters in a chain, as seen in figure \ref{fig:chain}.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=1.5in]{chain}
    \caption{Chain of Adapters structure
    after n versions have been published }
    \label{fig:chain}
\end{figure}

The method used in this technique may be used in our solution as well.
The availability of our system can be increased by employing this method as a
placeholder adaption mechanism while the adapter proxy is being built and initialized for earlier contract versions.

\section{API Management Tools} % (fold)
\label{sec:api_management_tools}

\section{Deployment Strategies} % (fold)
\label{sec:deployment_strategies}

