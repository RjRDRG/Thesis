%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter4.tex
%% NOVA thesis document file
%%
%% Chapter with lots of dummy text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter4.tex}%

\chapter{Related Work}
\label{cha:related_work}

As mentioned in Section 1, this work will encompass the development of
mechanisms for evaluating and managing the soundness of updates in service contracts.
We now summarize the most significant pieces of work from the literature
that relate to the evolution of microservices and their APIs.

\section{Serialization Frameworks} % (fold)
\label{sec:serialization_frameworks}

Serialization is the process of encoding system state into a standardized format so that it can be delivered across a network or stored with durability.
For serialization, each language usually provides a corresponding library, such as Java serialization.
In the setting of service oriented computing, serialization libraries supplied by programming languages are typically not used to encode messages between services,
because each service may be written in a different language. As a result, data consumers will be unable to comprehend data producers.
Cross-language serialization libraries, such as JSON, can solve this problem.
However, formats such as JSON lack a strictly defined structure,
making data consumption more difficult due to poor type-safety guarantees, and the ability for fields to be unilaterally added or withdrawn at any moment without the consumers' knowledge.
What's missing is a "schema" for data exchange between producers and consumers, akin to an API contract.

There have been a few cross-language serialization frameworks that require the data structure to be properly described via schemas.
XML, Avro, Thrift, and Protocol Buffers are among these libraries.
The benefit of having a schema is that it explicitly defines the data's structure, type, and meaning.

Such frameworks allow for schema evolution, but only to a limited extent:
\begin{itemize}
    \item Only the addition, removal and renaming of fields is allowed.
    \item To ensure backwards compatibility, added fields must be optional.
    \item The only fields that can be removed to maintain forward compatibility are optional fields.
    \item To provide both forward and backward compatibility, removed and added fields must be optional.
\end{itemize}
Backward compatibility refers to a consumer using schema X to process data produced by schema X-1,
whereas forward compatibility refers to data produced by schema X being read by consumers using schema X-1.

The need for optional fields in the above cases is due to a lack of information on the consumer, typically this information is supplied with the use of default values.
In order to support required fields with backwards-forwards compatibility, the system should enforce required fields when the consumer and producer agree on the schema,
and only use default values when the consumer and producer disagree on the schema.
These limitations are problematic because, in order to enforce required fields in former case,
validation logic would need to be written repeatedly by programmers (in the same layer as the business logic).
This validation should be in a layer above because, the scale of the validation logic is proportional to the complexity of messages being validated;
for messages with more properties, particularly those with nested objects, the validation footprint can rise dramatically in terms of both line-count and logical complexity.

These frameworks only tackle part of the problem of API evolution in microservices,
because they can only accommodate changes in the schema of records.
They can not manage changes in the API's signature, such as the method of a REST endpoint.
To eliminate potential mismatches and reduce the complexity of the evolution process, we believe it is critical to manage both sorts of evolution's in a single integrated approach.

\section{Schema Registry} % (fold)
\label{sec:schema_registry}

A schema registry, as the name implies, is a repository for schemas.
It stores a versioned history of schemas and provides an interface for retrieving, registering, as well as checking the compliance of schemas.
It is essentially a CRUD application with a RESTful API and persistent storage for schema definitions, where each schema is given a unique ID.

Schema registries are commonly used in situations where data consumers need to know the structure of the data written by producers at runtime.
A producer could send its schema to consumers along with the response to a request;
however, this is usually a bad idea because it would result in duplicating functionality across all services, making the system more difficult to maintain.

One framework that makes use of this system is Avro, a data serialization framework developed within Apache's Hadoop project.
The serialized byte sequence of each record in Avro does not include field metadata such as the field name or a tag, but merely the field value, allowing for a more compact serialization method.
All field values are appended back to back, in the same order as they appear in the schema \ref{fig:avro}.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=1.5in]{avro}
    \caption{A Avro schema and its associated serialized record }
    \label{fig:avro}
\end{figure}

The deserializer knows which bytes belong to which field by comparing both the reader and writer schemas and by matching fields by their names.

There are other systems that make use of registry's such a Java RMI that uses schema registry's
to perform remote method invocation, the object-oriented equivalent of remote procedure calls, with support for direct
transfer of serialized Java classes and distributed garbage-collection.

\section{Chain of adapters} % (fold)
\label{sec:chain_of_adapters}

The chain of adapters is a design strategy for providing a web service in the face of independently developed unsupervised clients while preserving strict backwards compatibility.
This approach decomposes long update/rollback transactions into smaller, independent transactions.
This technique is discussed in depth in the work [A Design for Adaptive Web Service Evolution]. The key concepts of the approach are presented below.

When a new service API is published, the old API is not decommissioned;
instead, it is made available in a different namespace;
in a RESTful system, this often entails supporting operations in different versions.
Endpoints belonging to the API in v1.0 can be made accessible via the path http://example.system/v1,
whereas endpoints belonging to the API in v2.0 are accessible via the path http://example.system/v2.

The previous API implementations are replaced by adapters that redirect all calls to the next API version implementation and also translate data structures as necessary.

Updating a service's API in this manner requires the programmer to do two additional steps:
\begin{itemize}
    \item Duplicate all modified endpoints in service contract into a different namespace.
    \item Implement an adapter that translates the endpoints in version vi->vi+1.
\end{itemize}

The same web service that supports the revised service endpoints also supports the older endpoints that require adapters.
It is not necessary to deploy the web service in several versions.

When a consumer is using version v1, and the producer is using version v4, the endpoints that provide the service in version v1 use the adapters v1->v2, v2->v3, and v3->v4 to translate the operation before forwarding it to the endpoint in the current version.

The service's backwards compatibility is ensured by the composition of adapters in a chain, as seen in figure \ref{fig:chain}.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=1.5in]{chain}
    \caption{Chain of Adapters structure
    after n versions have been published }
    \label{fig:chain}
\end{figure}

\section{Service Integration Adapters} % (fold)
\label{sec:service_integration_adapters}

There has been substantial investigation on the adaptability of services in the context of service-oriented architectures.
Although SOA and microservices are similar in that they both use a separation approach based on services,
these two designs differ in several important ways, most notably in that microservices are loosely coupled whereas SOA services are tightly tied due to common data storage between services.
As a result, adaptability in SOA is focused on service integration and replaceability rather than evolution.

In the setting of SOA, adaptors are used to wrap the various services which are in general heterogeneous,
(communicate with different protocols and support different data formats) so that they can appear as homogeneous and therefore easier to be integrated.

These adaptors presume that data adaptation has already been established by developers at an upper layer,
their primary concern is merging services with different communication protocols and mismatches between operations that have the
same functionality but differ in operation name, number, order or type of operation input/output parameters.

[Developing Adapters for Web Services Integration] presents a taxonomy of all conceivable mismatches as well as remedies to each type in the context of SOA.
Some of the mismatches defined remain important in the context of microservice evolution:
\begin{itemize}
    \item Message Split Mismatch

    This type of differences occurs when the protocol P requires a single message to achieve certain functionality, while in protocol PR the same behavior is achieved by receiving several messages.
    \item Message Merge Mismatch

    This type of differences occurs when protocol P needs to receive several messages for achieving certain
    functionality while protocol PR requires one message to achieve the same functionality.
    \item Differences at the Operation Level

    This type of mismatch occurs when the operation O of S imposes constraints on input parameters,
    which are less restrictive than those of OR input parameters in SR (e.g., differences in value ranges).
\end{itemize}

The two first types of mismatches can be handled in the context of microservice evolution without the use of adaptors.
Essentially, the old endpoints are kept operational but marked as deprecated until there are no more consumers using them, and the new endpoints are provided in parallel.

The latter form of mismatch has an impact on microservice evolution only if service contracts allow refined types.
In other words, when constraints in input parameter are validated in a layer above the application layer.
If validation is conducted in a lower layer, the mismatch can be easily resolved by modifying the application's implementation while keeping the service contract untouched.

\section{} % (fold)
\label{sec:serialization_frameworks}

\section{Serialization Frameworks} % (fold)
\label{sec:serialization_frameworks}