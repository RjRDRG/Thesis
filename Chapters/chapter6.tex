%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter4.tex
%% NOVA thesis document file
%%
%% Chapter with lots of dummy text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter4.tex}%

\chapter{Results}
\label{cha:results}

\section{Benchmark platform} % (fold)
\label{sec:benchmark_platform}

It was constructed a benchmark platform to compare our solution to existing ones.
This section discusses the proposed benchmark platform's requirements and design.
\citeauthor{microservices2017benchmark} \cite{microservices2017benchmark} propose an initial set of requirements
to support repeatable microservices research (Figure \ref{fig:benchmark}).

\begin{figure}[htbp]
    \centering
    \includegraphics[height=4.1in]{benchmark_requirements}
    \caption{Benchmark Requirements \cite{microservices2017benchmark}}
    \label{fig:benchmark}
\end{figure}

In addition to the requirements listed above, it was considered that the following requirements should also be included:
\begin{itemize}
    \setlength\itemsep{0em}
    \item The ability to evaluate different solutions in comparable scenarios while utilizing the same evaluation criteria;
    \item Evaluating solutions without requiring modifications to implementations;
    \item Experiments must be simple to share and reproduce by different individuals;
    \item It should be possible to aggregate reported metrics for a specific time period between two events, such as the start and end of a service's evolution;
    \item Users must be able to specify how and when each service should evolve via a configuration file or directly through a terminal.
\end{itemize}

The architecture of the benchmark platform can be seen in Figure \ref{fig:canvas}.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=6.2in]{canvas}
    \caption{Benchmark platform components}
    \label{fig:canvas}
\end{figure}

\newpage

The architecture components, and their applications are described below:

\paragraph{- Kubernetes \cite{kubernetes}} is the test environment.
It hosts the services holding the solutions implementations.
The loading testing scripts are also managed via services in Kubernetes.

\paragraph{- Promotheus \cite{turnbull2018monitoring}} is a pull-based monitoring system.
It periodically sends HTTP scrape requests, the response to these requests is parsed in storage along with the metrics for the scrape itself.
Prometheus provides a query language that allows the metrics to be aggregated by events, components or metadata.
The gathered metrics can be consulted by external tools or directly visualized in this platform via dashboards.

\paragraph{- Artillery} \textbf{TODO: reference}
is an open-source performance and reliability testing suite for developers and system administrators.

\paragraph{- Jenkins} \textbf{TODO: reference}
is a free and open automation server.
It aids in the automation of portions of software development such as building, testing, and deploying, allowing for continuous integration and continuous delivery.

\section{Evaluation Methodology} % (fold)
\label{sec:evaluation_methodology}

The setup and the methodology of the conducted experiments is described below:

\paragraph{- Environment -}

The experiments were conducted in a cloud environment with 10 virtual machines.
All 10 nodes were hosted in the same datacenter in the region of Strasbourg, France.
Each virtual machines was hosted in different physical machines, no two VMs shared the same hardware.
Each node is interconnected with a bandwidth of 2 Gb/s.
Each virtual machine is equipped with 4GB of RAM and 2 virtual cores of an Intel Core Haswell (no TSX) cpu.

\paragraph{- Tested microservice system -}

To test the cost of the adapter system a demo application was developed.
The demo application holds a configurable amount of services that run an identical image.
The image is a simple HTTP spring server that returns received requests unchanged.
The interconnections between microservices is configurable thorough environment variables.
The number of synchronous calls (SyC) between microservices for a single request and also the number of parallel calls (PrC) between microservices is configurable.
The demo application was developed in two separate versions with unique contracts.
In the first version, one of the procedures of the application was altered.
All the parameters in the body's schema of this procedure were renamed.

\paragraph{- Experiment Setup -}

To inject load in the experiments, 10 workers of the artillery load testing tool were used,
the workers are executed remotely in the cluster and were spread in 6 nodes.
The workers are supported by Kubernetes pods that are monitored by a common job.
The workers are installed and removed from cluster using the Artillery-Operator-Project \textbf{TODO: ref}, that enables
the creation and execution of distributed Artillery load tests from a Kubernetes cluster, at scale.
The architecture of this tool is depicted in Figure \ref{fig:artilleryoperator}.
In each experiment all the resources of previous experiments were deleted and a new job and namespace were created in the
cluster to avoid contamination in the metrics extracted.
The experiments were carried out sequentially with a 5-minute break in between.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3.5in]{artillery-operator-architecture}
    \caption{Artillery Operator Architecture TODO improve image}
    \label{fig:artilleryoperator}
\end{figure}

\paragraph{}

Each experiment had three phases:
a warm up phase of 30s with an arrival rate of 5 virtual users/second;
a ramp up phase of 2 minutes;
a sustained load phase of 6 minutes.
The metrics were only extracted during the sustained load phase.
The first and last 30s were left out to allow time for all the artillery worker threads to enter the sustained load phase.
It was excluded tests that had a high enough throughput to saturate the application.
The error rate and timeouts were zero during all the presented experiments.

\paragraph{- Metrics -}

In each experiment it was extracted 3 metrics: the cpu resources, the memory resources and latency of the system.
The resources were only queried from the adapters and the replicas that belong to the target microservice application under test.
The resources consumed by the monitoring system and artillery workers were excluded from the results.
The throughput and latency were extracted by parsing the reports of the artillery load testing tool.
The cpu resource samples were taken from each node at 5-second intervals using the prometheus node exporter.
Memory resource samples were gathered from each container at 5-second intervals using the kubelet node agent API.
The latency statistics were calculated with a period of 10s over 5 min, the results obtained represent the
average of the reported 30 periods.

\section{Experiments} % (fold)
\label{sec:experiments}

Four experiments were carried out in order to test the implementation of the prototype:
the point-to-point experiment, the weighted point-to-point experiment, the bi-partition experiment, and the live contract evolution experiment.
The experiments are detailed below.

\subsection{Point to Point Experiment}

The topology of this experiment is represent in Figure \ref{fig:point}.
The primary goal of this experiment was to assess the overhead of the adapter in terms of latency and computational resources.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=2.4in]{pointExperiment}
    \caption{Point-to-point experiment}
    \label{fig:point}
\end{figure}

The experiment is composed by two microservices:
the first receives requests from artillery workers and forwards them to the second microservice, which responds with the request.
It was first established a baseline of comparison
by doing the tests without the adapter, and then, the adapter was introduced and the tests were repeated.

\paragraph{}

The measured cpu overhead of the adapter over a
range of request rates is depicted in Figure \ref{fig:cpuPtp}.
The overhead was calculated by processing the averages of the measured CPU resources in the baseline and adapter tests, and then by dividing the two averages.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3in]{Results/PtP/Cpu-PtP}
    \caption{Adapter CPU overhead in the point-to-point experiment - 100 bytes (payload size)}
    \label{fig:cpuPtp}
\end{figure}

\paragraph{}

In this experiment it is expected to have an overhead of 150\% because: the adapter and the microservices share the same request rate;
the cost of the adapter is expected to be equal or to the cost of the tested microservices.
The tested microservices were implemented with the same frameworks as the adapter, consequently adding the adapter will be equivalent to adding one more microservice to the existing two.

\paragraph{}

As can be seen in the results shown Figure \ref{fig:cpuPtp} the measured overhead doesn't fall to far outside our expectations, with the exception of the first data point.
The overhead 84\% is explainable by the configuration of the containers:
the adapter and demo application containers were initialized in Kubernetes with a default of 100 millicpu (10\% of tested cpu core),
this cpu resources far exceed the capacity necessary to support a request rate 60 requests/second,
consequently the container's cpu scheduling was throttle-down after start up;
the overhead was inferior to 100\% because the scheduled cpu resources in the experiment with the adapter, were throttled down faster than in the baseline experiment without the adapter.

\paragraph{}

Figure \ref{fig:memPtp} shows the measured memory overhead of the adapter over a
range of request rates.
The overhead was calculated by processing the averages of the measured memory resources in the baseline and adapter tests, and then by dividing the two averages.

\paragraph{}

The average cost of 190\% in Figure \ref{fig:memPtp} is understandable because: although being stateless, the adapter stores two transient messages in memory for each request, the request message and the adapted request message;
the demo application only keeps one request in memory, it stands to reason that the adapter memory cost is roughly double of the demo application.
As can be seen in Figure \ref{fig:memCostPtp}, the used memory of the adapter grows linearly with the request rate, as it was expected.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3.2in]{Results/PtP/Memory-PtP}
    \caption{Adapter memory overhead in the point-to-point experiment - 100 bytes (payload size)}
    \label{fig:memPtp}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3.2in]{Results/PtP/MemoryCost_PtP}
    \caption{Adapter memory cost in the point-to-point experiment - 100 bytes (payload size)}
    \label{fig:memCostPtp}
\end{figure}

\newpage

The latency overhead of the adapter in the point-to-point experiment is depicted in Figure \ref{fig:latPtP}.
The overhead was calculated by processing the averages of the measured latency in the baseline and adapter tests, and then by dividing the two averages.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3.4in]{Results/PtP/Latency-PtP}
    \caption{Adapter latency cost in the point-to-point experiment - 100 bytes (payload size)}
    \label{fig:latPtP}
\end{figure}

It is expected for the overhead to be less or equal to 150\%, because the latency overhead is largely attributable to communication costs.
The adapter introduces an additional communication step ''app1->adapter->app2'' to the baseline communication chain that already has two steps ''workers->app1'' and ''app1->app2''.
The introduced communication step is expected to have a slightly inferior cost to the other steps, as it is performed via interprocess-calls, while the others are done via a private network with 2 GB of bandwidth.
The hypothesis is confirmed by the experiment results in Figure \ref{fig:latPtP}.

\paragraph{}

The Figures  \ref{fig:payloadCpuPtP} and \ref{fig:payloadLatPtP} illustrate how larger payload sizes effect the latency and the consumed cpu resources.
A request rate of 480 request per second was used in this experiment.
The latency is expected to rise with larger payload sizes because the HTTP packet size is typically 1.5 KB, bigger messages will need to be sent in multiple packets.
The cpu cost is also expected to rise because the cost of deserialization of messages grows with the message payload size.

\paragraph{}

The results illustrated don't reveal any difference for the tested range of payload sizes.
This can be explainable because the tested payload size range of 100 bytes to 2 KB each fits under a single HTTP packet.
This range has initially selected because most services use http messages that fall under this range.
In future work this experiment should be repeated for larger payload sizes, up to 2 MB, to see how the adapter scales in this scenario.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3.4in]{Results/PtP_Size/Cpu-PtP_Size}
    \caption{Adapter CPU cost in the point-to-point experiment - 480 request/s (arrival rate)}
    \label{fig:payloadCpuPtP}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3in]{Results/PtP_Size/Latency-PtP_Size}
    \caption{Adapter Latency cost in the point-to-point experiment - 480 request/s (arrival rate)}
    \label{fig:payloadLatPtP}
\end{figure}

\subsection{Weighted Point to Point Experiment}

The topology of this experiment is represent in Figure \ref{fig:routExp}.
This experiment is also composed by the two microservices, where the first microservice forwards a percentage of its request thorough the adapter
and forwards its remaining requests directly to the other microservice.
The primary goal of this experiment was to see if the adapter overhead is significantly lower in scenarios where only a small percentage of all requests require adaptation.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=1.8in]{routingExperiment}
    \caption{Weighted point to point experiment}
    \label{fig:routExp}
\end{figure}

Figure \ref{fig:routCpu} depicts the measured CPU cost of the adapter, the horizontal axis represents the percentage of requests that need adaptation, and the vertical axis represents the
cpu cores utilized by the adapter and demo applications.

\textbf{TODO. recolor image to blue and legend}

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3in]{Results/R/Cpu-R}
    \caption{CPU used in the routing experiment - 480 request/s (arrival rate) 100 bytes (payload size)}
    \label{fig:routCpu}
\end{figure}

In this experiment it is expected for the overhead to steadily increase because the arrival rate will increase, as the adapter receives a higher percentage of requests. To support higher request rates
the adapter implementation will need to launch more client threads which, consequently, increase the consumed cpu resources.
    
\paragraph{}

When comparing the first and last data points in Figure \ref{fig:routCpu}, we can see that consumed resources have doubled, as expected.
In the other data points, the overhead remains relatively constant with a margin of error of 20\%,
this explainable because the default amount of allocated threads by Tomcat (200 threads) is sufficient to handle the arrival rate in these data points.
When 50\% of requests are routed through the adapter, the arrival rate is 240 requests/s, the tomcat thread model uses a thread per request,
consequently 240 threads are necessary to handle the arrival rate, which can be translated to an overhead of 120\%.

\paragraph{}

The 20\% margin of error in the other data points is explainable by the nondeterministic nature of the artillery workers.
Remark that the artillery test has two scenarios: one where the worker calls an endpoint that can only be handled by using the adapter, and another in which the adapter is not utilized.
Each scenario weight as set to reflect the corresponding test percentages,
however, the distribution of requests is nondeterministic and will only be equal to the given weights over very long test periods.
The test time for this experiment was set to 5 minutes for each data point.

\paragraph{}

The utilized memory of the adapter and demo applications in this experiment is depicted in Figure \ref{fig:routMem}.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3in]{Results/R/Memory-R}
    \caption{Memory used in the routing experiment - 480 request/s (arrival rate) 100 bytes (payload size)}
    \label{fig:routMem}
\end{figure}

It is expected for the used memory to increase linearly with the fraction of requests routed through the adapter because, as discussed previously,
the adapter implementation spatial complexity is equivalent to double of the demo application implementation spatial complexity.
The results obtained confirm the expectation.

\paragraph{}

Figure \ref{fig:routLatency} shows the request latency as the proportion of adapted requests increases.
It is expected that the latency will steadily increase, because of the same reasons previously stated.
When comparing the first and last data points, we can see that the latency has doubled as expected.
In the other data points, the latency remains relatively constant, this mainly due to the error introduced by the short test duration.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3in]{Results/R/Latency-R}
    \caption{Request latency in the routing experiment - 480 request/s (arrival rate) 100 bytes (payload size)}
    \label{fig:routLatency}
\end{figure}

\subsection{Bi-partition Experiment}

The goal of the experiment was to see if the conclusions obtained from the point-to-point experiment are also applicable in scenarios with multiple microservices.
The experiment is composed by two disjoint sets of microservices, where the first set receives request from the artillery workers and forward the requests to the second of microservices.
The topology is represent in Figure \ref{fig:biPart}.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=2.4in]{bipartExperiment}
    \caption{Bi-partition experiment}
    \label{fig:biPart}
\end{figure}

\paragraph{}

First, it was established a baseline of comparison by doing the tests without the adapter,
then, it was upgraded all the microservices in the second set and added adapters to support communication across the disjoint sets.

\paragraph{}

The outcomes of the ''point-to-point'' and ''bi-partition'' experiments are equivalent, as seen in the Figures \ref{fig:biPartCPU}, \ref{fig:biPartMem} and \ref{fig:biPartLatency}.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3in]{Results/Bp/Cpu-Bp}
    \caption{Adapter CPU overhead in the bi-partition experiment \\\hspace{\textwidth} 100 bytes (payload size)}
    \label{fig:biPartCPU}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3in]{Results/Bp/Memory-Bp}
    \caption{Adapter memory overhead in the bi-partition experiment \\\hspace{\textwidth} 100 bytes (payload size)}
    \label{fig:biPartMem}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[height=3in]{Results/Bp/Latency-Bp}
    \caption{Adapter latency overhead in the bi-partition experiment \\\hspace{\textwidth} 100 bytes (payload size)}
    \label{fig:biPartLatency}
\end{figure}

\subsection{Live Contract Evolution Experiment}

\textbf{TODO}